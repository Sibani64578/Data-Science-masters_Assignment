{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e524c05-98dc-43e3-86bb-d558accd0a83",
   "metadata": {},
   "source": [
    "# Q1. Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic regression would be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2956e7b-0ea9-4e0c-9a96-1d80538f94d7",
   "metadata": {},
   "source": [
    "Linear Regression: Linear regression is used when the dependent variable (the one you're trying to predict) is continuous. It's suitable for problems where you want to predict a value within a range, such as predicting house prices or stock prices.\n",
    "Logistic Regression: Logistic regression is used when the dependent variable is categorical. It's specifically designed for binary classification problems where the outcome is either 0 or 1 (e.g., yes/no, spam/ham, pass/fail).\n",
    "Output Function:\n",
    "\n",
    "Linear Regression: It uses a linear function to model the relationship between the independent variables and the continuous dependent variable. The output can be any real number.\n",
    "Logistic Regression: It uses the logistic (sigmoid) function to model the relationship between the independent variables and the probability of the dependent variable being 1. The output is a probability value between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e941625f-1168-4f0e-92ee-62a12e6737a6",
   "metadata": {},
   "source": [
    "# Q2. What is the cost function used in logistic regression, and how is it optimized?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b463ed6-0b92-4720-be8f-f9d5ed512a78",
   "metadata": {},
   "source": [
    "The cost function for logistic regression is often referred to as the \"log loss\" or \"cross-entropy loss.\" It is defined as follows for a single training example:\n",
    "\n",
    "Cost(y,y^​)=−[ylog( y^​ )+(1−y)log(1− y^​ )]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d89d979-ba54-4cf0-8d33-f1689cf709ce",
   "metadata": {},
   "source": [
    "# Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a36df8b-6b86-457b-b218-b7cdaee1c8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization is a technique used in logistic regression (and other machine learning models) to prevent overfitting, which occurs when a model fits the training data too closely and captures noise or random fluctuations rather than the underlying patterns. Overfit models perform well on the training data but generalize poorly to new, unseen data. Regularization helps strike a balance between fitting the training data well and having a model that generalizes effectively to new data\n",
    "L2 Regularization (Ridge):\n",
    "\n",
    "In L2 regularization, a penalty term is added to the logistic regression cost function as well, but this time, the penalty is proportional to the square of the model coefficients.\n",
    "The L2 regularization term is represented as \n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "�\n",
    "2\n",
    "λ∑ \n",
    "j=1\n",
    "n\n",
    "​\n",
    " θ \n",
    "j\n",
    "2\n",
    "​\n",
    " , where \n",
    "�\n",
    "λ is the regularization parameter and \n",
    "�\n",
    "n is the number of features.\n",
    "L2 regularization encourages the model to keep all features but shrink the coefficients towards zero. It discourages large coefficient values.\n",
    "This can help prevent overfitting by reducing the impact of individual features on the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91813067-55c4-4f94-8f3b-28a95edd0cdb",
   "metadata": {},
   "source": [
    "# Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a5983c4-d126-4085-a4f1-11189082e89a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+200B (2872341640.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 12\u001b[0;36m\u001b[0m\n\u001b[0;31m    ​\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid non-printable character U+200B\n"
     ]
    }
   ],
   "source": [
    "True Positive Rate (TPR): TPR, also known as sensitivity or recall, measures the proportion of actual positive cases that are correctly predicted as positive by the model. It is calculated as:\n",
    "\n",
    "TPR\n",
    "=\n",
    "True Positives\n",
    "True Positives\n",
    "+\n",
    "False Negatives\n",
    "TPR= \n",
    "True Positives+False Negatives\n",
    "True Positives\n",
    "​\n",
    " \n",
    "TPR represents how well the model identifies positive cases when they are present in the dataset.\n",
    "\n",
    "False Positive Rate (FPR): FPR measures the proportion of actual negative cases that are incorrectly predicted as positive by the model. It is calculated as:\n",
    "\n",
    "FPR\n",
    "=\n",
    "False Positives\n",
    "False Positives\n",
    "+\n",
    "True Negatives\n",
    "]\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "ℎ\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "′\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    ".\n",
    "FPR= \n",
    "False Positives+True Negatives\n",
    "False Positives\n",
    "​\n",
    " ]FPRrepresentsthemodel \n",
    "′\n",
    " stendencytomakefalsealarmsbyclassifyingnegativecasesaspositive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dcb206-1765-4c07-95d0-05857e0d9054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
